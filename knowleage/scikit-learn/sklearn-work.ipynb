{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LogisticRegressionCV源码\n",
    "```python\n",
    "class LogisticRegressionCV(LogisticRegression, LinearClassifierMixin, BaseEstimator):\n",
    "    \"\"\"Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
    "\n",
    "    See glossary entry for :term:`cross-validation estimator`.\n",
    "\n",
    "    This class implements logistic regression using liblinear, newton-cg, sag\n",
    "    or lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n",
    "    regularization with primal formulation. The liblinear solver supports both\n",
    "    L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n",
    "    Elastic-Net penalty is only supported by the saga solver.\n",
    "\n",
    "    For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n",
    "    is selected by the cross-validator\n",
    "    :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n",
    "    using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "    solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n",
    "\n",
    "    Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Cs : int or list of floats, default=10\n",
    "        Each of the values in Cs describes the inverse of regularization\n",
    "        strength. If Cs is as an int, then a grid of Cs values are chosen\n",
    "        in a logarithmic scale between 1e-4 and 1e4.\n",
    "        Like in support vector machines, smaller values specify stronger\n",
    "        regularization.\n",
    "\n",
    "    fit_intercept : bool, default=True\n",
    "        Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "        added to the decision function.\n",
    "\n",
    "    cv : int or cross-validation generator, default=None\n",
    "        The default cross-validation generator used is Stratified K-Folds.\n",
    "        If an integer is provided, then it is the number of folds used.\n",
    "        See the module :mod:`sklearn.model_selection` module for the\n",
    "        list of possible cross-validation objects.\n",
    "\n",
    "        .. versionchanged:: 0.22\n",
    "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
    "\n",
    "    dual : bool, default=False\n",
    "        Dual (constrained) or primal (regularized, see also\n",
    "        :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n",
    "        is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n",
    "        n_samples > n_features.\n",
    "\n",
    "    penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n",
    "        Specify the norm of the penalty:\n",
    "\n",
    "        - `'l2'`: add a L2 penalty term (used by default);\n",
    "        - `'l1'`: add a L1 penalty term;\n",
    "        - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
    "\n",
    "        .. warning::\n",
    "           Some penalties may not work with some solvers. See the parameter\n",
    "           `solver` below, to know the compatibility between the penalty and\n",
    "           solver.\n",
    "\n",
    "    scoring : str or callable, default=None\n",
    "        A string (see :ref:`scoring_parameter`) or\n",
    "        a scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``. For a list of scoring functions\n",
    "        that can be used, look at :mod:`sklearn.metrics`. The\n",
    "        default scoring option used is 'accuracy'.\n",
    "\n",
    "    solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, \\\n",
    "            default='lbfgs'\n",
    "\n",
    "        Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
    "        To choose a solver, you might want to consider the following aspects:\n",
    "\n",
    "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
    "          and 'saga' are faster for large ones;\n",
    "        - For multiclass problems, all solvers except 'liblinear' minimize the full\n",
    "          multinomial loss;\n",
    "        - 'liblinear' might be slower in :class:`LogisticRegressionCV`\n",
    "          because it does not handle warm-starting.\n",
    "        - 'liblinear' can only handle binary classification by default. To apply a\n",
    "          one-versus-rest scheme for the multiclass setting one can wrap it with the\n",
    "          :class:`~sklearn.multiclass.OneVsRestClassifier`.\n",
    "        - 'newton-cholesky' is a good choice for\n",
    "          `n_samples` >> `n_features * n_classes`, especially with one-hot encoded\n",
    "          categorical features with rare categories. Be aware that the memory usage\n",
    "          of this solver has a quadratic dependency on `n_features * n_classes`\n",
    "          because it explicitly computes the full Hessian matrix.\n",
    "\n",
    "        .. warning::\n",
    "           The choice of the algorithm depends on the penalty chosen and on\n",
    "           (multinomial) multiclass support:\n",
    "\n",
    "           ================= ============================== ======================\n",
    "           solver            penalty                        multinomial multiclass\n",
    "           ================= ============================== ======================\n",
    "           'lbfgs'           'l2'                           yes\n",
    "           'liblinear'       'l1', 'l2'                     no\n",
    "           'newton-cg'       'l2'                           yes\n",
    "           'newton-cholesky' 'l2',                          no\n",
    "           'sag'             'l2',                          yes\n",
    "           'saga'            'elasticnet', 'l1', 'l2'       yes\n",
    "           ================= ============================== ======================\n",
    "\n",
    "        .. note::\n",
    "           'sag' and 'saga' fast convergence is only guaranteed on features\n",
    "           with approximately the same scale. You can preprocess the data with\n",
    "           a scaler from :mod:`sklearn.preprocessing`.\n",
    "\n",
    "        .. versionadded:: 0.17\n",
    "           Stochastic Average Gradient descent solver.\n",
    "        .. versionadded:: 0.19\n",
    "           SAGA solver.\n",
    "        .. versionadded:: 1.2\n",
    "           newton-cholesky solver.\n",
    "\n",
    "    tol : float, default=1e-4\n",
    "        Tolerance for stopping criteria.\n",
    "\n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations of the optimization algorithm.\n",
    "\n",
    "    class_weight : dict or 'balanced', default=None\n",
    "        Weights associated with classes in the form ``{class_label: weight}``.\n",
    "        If not given, all classes are supposed to have weight one.\n",
    "\n",
    "        The \"balanced\" mode uses the values of y to automatically adjust\n",
    "        weights inversely proportional to class frequencies in the input data\n",
    "        as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "        Note that these weights will be multiplied with sample_weight (passed\n",
    "        through the fit method) if sample_weight is specified.\n",
    "\n",
    "        .. versionadded:: 0.17\n",
    "           class_weight == 'balanced'\n",
    "\n",
    "    n_jobs : int, default=None\n",
    "        Number of CPU cores used during the cross-validation loop.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    verbose : int, default=0\n",
    "        For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n",
    "        positive number for verbosity.\n",
    "\n",
    "    refit : bool, default=True\n",
    "        If set to True, the scores are averaged across all folds, and the\n",
    "        coefs and the C that corresponds to the best score is taken, and a\n",
    "        final refit is done using these parameters.\n",
    "        Otherwise the coefs, intercepts and C that correspond to the\n",
    "        best scores across folds are averaged.\n",
    "\n",
    "    intercept_scaling : float, default=1\n",
    "        Useful only when the solver 'liblinear' is used\n",
    "        and self.fit_intercept is set to True. In this case, x becomes\n",
    "        [x, self.intercept_scaling],\n",
    "        i.e. a \"synthetic\" feature with constant value equal to\n",
    "        intercept_scaling is appended to the instance vector.\n",
    "        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
    "\n",
    "        Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "        as all other features.\n",
    "        To lessen the effect of regularization on synthetic feature weight\n",
    "        (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "    multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n",
    "        If the option chosen is 'ovr', then a binary problem is fit for each\n",
    "        label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
    "        across the entire probability distribution, *even when the data is\n",
    "        binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
    "        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
    "        and otherwise selects 'multinomial'.\n",
    "\n",
    "        .. versionadded:: 0.18\n",
    "           Stochastic Average Gradient descent solver for 'multinomial' case.\n",
    "        .. versionchanged:: 0.22\n",
    "            Default changed from 'ovr' to 'auto' in 0.22.\n",
    "        .. deprecated:: 1.5\n",
    "           ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.\n",
    "           From then on, the recommended 'multinomial' will always be used for\n",
    "           `n_classes >= 3`.\n",
    "           Solvers that do not support 'multinomial' will raise an error.\n",
    "           Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegressionCV())` if you\n",
    "           still want to use OvR.\n",
    "\n",
    "    random_state : int, RandomState instance, default=None\n",
    "        Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n",
    "        Note that this only applies to the solver and not the cross-validation\n",
    "        generator. See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "    l1_ratios : list of float, default=None\n",
    "        The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n",
    "        Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n",
    "        using ``penalty='l2'``, while 1 is equivalent to using\n",
    "        ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n",
    "        of L1 and L2.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : ndarray of shape (n_classes, )\n",
    "        A list of class labels known to the classifier.\n",
    "\n",
    "    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "        Coefficient of the features in the decision function.\n",
    "\n",
    "        `coef_` is of shape (1, n_features) when the given problem\n",
    "        is binary.\n",
    "\n",
    "    intercept_ : ndarray of shape (1,) or (n_classes,)\n",
    "        Intercept (a.k.a. bias) added to the decision function.\n",
    "\n",
    "        If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "        `intercept_` is of shape(1,) when the problem is binary.\n",
    "\n",
    "    Cs_ : ndarray of shape (n_cs)\n",
    "        Array of C i.e. inverse of regularization parameter values used\n",
    "        for cross-validation.\n",
    "\n",
    "    l1_ratios_ : ndarray of shape (n_l1_ratios)\n",
    "        Array of l1_ratios used for cross-validation. If no l1_ratio is used\n",
    "        (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n",
    "\n",
    "    coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or \\\n",
    "                   (n_folds, n_cs, n_features + 1)\n",
    "        dict with classes as the keys, and the path of coefficients obtained\n",
    "        during cross-validating across each fold and then across each Cs\n",
    "        after doing an OvR for the corresponding class as values.\n",
    "        If the 'multi_class' option is set to 'multinomial', then\n",
    "        the coefs_paths are the coefficients corresponding to each class.\n",
    "        Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n",
    "        ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n",
    "        intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n",
    "        ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n",
    "        ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n",
    "\n",
    "    scores_ : dict\n",
    "        dict with classes as the keys, and the values as the\n",
    "        grid of scores obtained during cross-validating each fold, after doing\n",
    "        an OvR for the corresponding class. If the 'multi_class' option\n",
    "        given is 'multinomial' then the same scores are repeated across\n",
    "        all classes, since this is the multinomial class. Each dict value\n",
    "        has shape ``(n_folds, n_cs)`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n",
    "        ``penalty='elasticnet'``.\n",
    "\n",
    "    C_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
    "        Array of C that maps to the best scores across every class. If refit is\n",
    "        set to False, then for each class, the best C is the average of the\n",
    "        C's that correspond to the best scores for each fold.\n",
    "        `C_` is of shape(n_classes,) when the problem is binary.\n",
    "\n",
    "    l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
    "        Array of l1_ratio that maps to the best scores across every class. If\n",
    "        refit is set to False, then for each class, the best l1_ratio is the\n",
    "        average of the l1_ratio's that correspond to the best scores for each\n",
    "        fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n",
    "\n",
    "    n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n",
    "        Actual number of iterations for all classes, folds and Cs.\n",
    "        In the binary or multinomial cases, the first dimension is equal to 1.\n",
    "        If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n",
    "        n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n",
    "\n",
    "    n_features_in_ : int\n",
    "        Number of features seen during :term:`fit`.\n",
    "\n",
    "        .. versionadded:: 0.24\n",
    "\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during :term:`fit`. Defined only when `X`\n",
    "        has feature names that are all strings.\n",
    "\n",
    "        .. versionadded:: 1.0\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    LogisticRegression : Logistic regression without tuning the\n",
    "        hyperparameter `C`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.linear_model import LogisticRegressionCV\n",
    "    >>> X, y = load_iris(return_X_y=True)\n",
    "    >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
    "    >>> clf.predict(X[:2, :])\n",
    "    array([0, 0])\n",
    "    >>> clf.predict_proba(X[:2, :]).shape\n",
    "    (2, 3)\n",
    "    >>> clf.score(X, y)\n",
    "    0.98...\n",
    "    \"\"\"\n",
    "\n",
    "    _parameter_constraints: dict = {**LogisticRegression._parameter_constraints}\n",
    "\n",
    "    for param in [\"C\", \"warm_start\", \"l1_ratio\"]:\n",
    "        _parameter_constraints.pop(param)\n",
    "\n",
    "    _parameter_constraints.update(\n",
    "        {\n",
    "            \"Cs\": [Interval(Integral, 1, None, closed=\"left\"), \"array-like\"],\n",
    "            \"cv\": [\"cv_object\"],\n",
    "            \"scoring\": [StrOptions(set(get_scorer_names())), callable, None],\n",
    "            \"l1_ratios\": [\"array-like\", None],\n",
    "            \"refit\": [\"boolean\"],\n",
    "            \"penalty\": [StrOptions({\"l1\", \"l2\", \"elasticnet\"})],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        Cs=10,\n",
    "        fit_intercept=True,\n",
    "        cv=None,\n",
    "        dual=False,\n",
    "        penalty=\"l2\",\n",
    "        scoring=None,\n",
    "        solver=\"lbfgs\",\n",
    "        tol=1e-4,\n",
    "        max_iter=100,\n",
    "        class_weight=None,\n",
    "        n_jobs=None,\n",
    "        verbose=0,\n",
    "        refit=True,\n",
    "        intercept_scaling=1.0,\n",
    "        multi_class=\"deprecated\",\n",
    "        random_state=None,\n",
    "        l1_ratios=None,\n",
    "    ):\n",
    "        self.Cs = Cs\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.cv = cv\n",
    "        self.dual = dual\n",
    "        self.penalty = penalty\n",
    "        self.scoring = scoring\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.class_weight = class_weight\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        self.solver = solver\n",
    "        self.refit = refit\n",
    "        self.intercept_scaling = intercept_scaling\n",
    "        self.multi_class = multi_class\n",
    "        self.random_state = random_state\n",
    "        self.l1_ratios = l1_ratios\n",
    "\n",
    "    @_fit_context(prefer_skip_nested_validation=True)\n",
    "    def fit(self, X, y, sample_weight=None, **params):\n",
    "        \"\"\"Fit the model according to the given training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Training vector, where `n_samples` is the number of samples and\n",
    "            `n_features` is the number of features.\n",
    "\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target vector relative to X.\n",
    "\n",
    "        sample_weight : array-like of shape (n_samples,) default=None\n",
    "            Array of weights that are assigned to individual samples.\n",
    "            If not provided, then each sample is given unit weight.\n",
    "\n",
    "        **params : dict\n",
    "            Parameters to pass to the underlying splitter and scorer.\n",
    "\n",
    "            .. versionadded:: 1.4\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted LogisticRegressionCV estimator.\n",
    "        \"\"\"\n",
    "        _raise_for_params(params, self, \"fit\")\n",
    "\n",
    "        solver = _check_solver(self.solver, self.penalty, self.dual)\n",
    "\n",
    "        if self.penalty == \"elasticnet\":\n",
    "            if (\n",
    "                self.l1_ratios is None\n",
    "                or len(self.l1_ratios) == 0\n",
    "                or any(\n",
    "                    (\n",
    "                        not isinstance(l1_ratio, numbers.Number)\n",
    "                        or l1_ratio < 0\n",
    "                        or l1_ratio > 1\n",
    "                    )\n",
    "                    for l1_ratio in self.l1_ratios\n",
    "                )\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"l1_ratios must be a list of numbers between \"\n",
    "                    \"0 and 1; got (l1_ratios=%r)\" % self.l1_ratios\n",
    "                )\n",
    "            l1_ratios_ = self.l1_ratios\n",
    "        else:\n",
    "            if self.l1_ratios is not None:\n",
    "                warnings.warn(\n",
    "                    \"l1_ratios parameter is only used when penalty \"\n",
    "                    \"is 'elasticnet'. Got (penalty={})\".format(self.penalty)\n",
    "                )\n",
    "\n",
    "            l1_ratios_ = [None]\n",
    "\n",
    "        X, y = validate_data(\n",
    "            self,\n",
    "            X,\n",
    "            y,\n",
    "            accept_sparse=\"csr\",\n",
    "            dtype=np.float64,\n",
    "            order=\"C\",\n",
    "            accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"],\n",
    "        )\n",
    "        check_classification_targets(y)\n",
    "\n",
    "        class_weight = self.class_weight\n",
    "\n",
    "        # Encode for string labels\n",
    "        label_encoder = LabelEncoder().fit(y)\n",
    "        y = label_encoder.transform(y)\n",
    "        if isinstance(class_weight, dict):\n",
    "            class_weight = {\n",
    "                label_encoder.transform([cls])[0]: v for cls, v in class_weight.items()\n",
    "            }\n",
    "\n",
    "        # The original class labels\n",
    "        classes = self.classes_ = label_encoder.classes_\n",
    "        encoded_labels = label_encoder.transform(label_encoder.classes_)\n",
    "\n",
    "        # TODO(1.7) remove multi_class\n",
    "        multi_class = self.multi_class\n",
    "        if self.multi_class == \"multinomial\" and len(self.classes_) == 2:\n",
    "            warnings.warn(\n",
    "                (\n",
    "                    \"'multi_class' was deprecated in version 1.5 and will be removed in\"\n",
    "                    \" 1.7. From then on, binary problems will be fit as proper binary \"\n",
    "                    \" logistic regression models (as if multi_class='ovr' were set).\"\n",
    "                    \" Leave it to its default value to avoid this warning.\"\n",
    "                ),\n",
    "                FutureWarning,\n",
    "            )\n",
    "        elif self.multi_class in (\"multinomial\", \"auto\"):\n",
    "            warnings.warn(\n",
    "                (\n",
    "                    \"'multi_class' was deprecated in version 1.5 and will be removed in\"\n",
    "                    \" 1.7. From then on, it will always use 'multinomial'.\"\n",
    "                    \" Leave it to its default value to avoid this warning.\"\n",
    "                ),\n",
    "                FutureWarning,\n",
    "            )\n",
    "        elif self.multi_class == \"ovr\":\n",
    "            warnings.warn(\n",
    "                (\n",
    "                    \"'multi_class' was deprecated in version 1.5 and will be removed in\"\n",
    "                    \" 1.7. Use OneVsRestClassifier(LogisticRegressionCV(..)) instead.\"\n",
    "                    \" Leave it to its default value to avoid this warning.\"\n",
    "                ),\n",
    "                FutureWarning,\n",
    "            )\n",
    "        else:\n",
    "            # Set to old default value.\n",
    "            multi_class = \"auto\"\n",
    "        multi_class = _check_multi_class(multi_class, solver, len(classes))\n",
    "\n",
    "        if solver in [\"sag\", \"saga\"]:\n",
    "            max_squared_sum = row_norms(X, squared=True).max()\n",
    "        else:\n",
    "            max_squared_sum = None\n",
    "\n",
    "        if _routing_enabled():\n",
    "            routed_params = process_routing(\n",
    "                self,\n",
    "                \"fit\",\n",
    "                sample_weight=sample_weight,\n",
    "                **params,\n",
    "            )\n",
    "        else:\n",
    "            routed_params = Bunch()\n",
    "            routed_params.splitter = Bunch(split={})\n",
    "            routed_params.scorer = Bunch(score=params)\n",
    "            if sample_weight is not None:\n",
    "                routed_params.scorer.score[\"sample_weight\"] = sample_weight\n",
    "\n",
    "        # init cross-validation generator\n",
    "        cv = check_cv(self.cv, y, classifier=True)\n",
    "        folds = list(cv.split(X, y, **routed_params.splitter.split))\n",
    "\n",
    "        # Use the label encoded classes\n",
    "        n_classes = len(encoded_labels)\n",
    "\n",
    "        if n_classes < 2:\n",
    "            raise ValueError(\n",
    "                \"This solver needs samples of at least 2 classes\"\n",
    "                \" in the data, but the data contains only one\"\n",
    "                \" class: %r\" % classes[0]\n",
    "            )\n",
    "\n",
    "        if n_classes == 2:\n",
    "            # OvR in case of binary problems is as good as fitting\n",
    "            # the higher label\n",
    "            n_classes = 1\n",
    "            encoded_labels = encoded_labels[1:]\n",
    "            classes = classes[1:]\n",
    "\n",
    "        # We need this hack to iterate only once over labels, in the case of\n",
    "        # multi_class = multinomial, without changing the value of the labels.\n",
    "        if multi_class == \"multinomial\":\n",
    "            iter_encoded_labels = iter_classes = [None]\n",
    "        else:\n",
    "            iter_encoded_labels = encoded_labels\n",
    "            iter_classes = classes\n",
    "\n",
    "        # compute the class weights for the entire dataset y\n",
    "        if class_weight == \"balanced\":\n",
    "            class_weight = compute_class_weight(\n",
    "                class_weight, classes=np.arange(len(self.classes_)), y=y\n",
    "            )\n",
    "            class_weight = dict(enumerate(class_weight))\n",
    "\n",
    "        path_func = delayed(_log_reg_scoring_path)\n",
    "\n",
    "        # The SAG solver releases the GIL so it's more efficient to use\n",
    "        # threads for this solver.\n",
    "        if self.solver in [\"sag\", \"saga\"]:\n",
    "            prefer = \"threads\"\n",
    "        else:\n",
    "            prefer = \"processes\"\n",
    "\n",
    "        fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
    "            path_func(\n",
    "                X,\n",
    "                y,\n",
    "                train,\n",
    "                test,\n",
    "                pos_class=label,\n",
    "                Cs=self.Cs,\n",
    "                fit_intercept=self.fit_intercept,\n",
    "                penalty=self.penalty,\n",
    "                dual=self.dual,\n",
    "                solver=solver,\n",
    "                tol=self.tol,\n",
    "                max_iter=self.max_iter,\n",
    "                verbose=self.verbose,\n",
    "                class_weight=class_weight,\n",
    "                scoring=self.scoring,\n",
    "                multi_class=multi_class,\n",
    "                intercept_scaling=self.intercept_scaling,\n",
    "                random_state=self.random_state,\n",
    "                max_squared_sum=max_squared_sum,\n",
    "                sample_weight=sample_weight,\n",
    "                l1_ratio=l1_ratio,\n",
    "                score_params=routed_params.scorer.score,\n",
    "            )\n",
    "            for label in iter_encoded_labels\n",
    "            for train, test in folds\n",
    "            for l1_ratio in l1_ratios_\n",
    "        )\n",
    "\n",
    "        # _log_reg_scoring_path will output different shapes depending on the\n",
    "        # multi_class param, so we need to reshape the outputs accordingly.\n",
    "        # Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the\n",
    "        # rows are equal, so we just take the first one.\n",
    "        # After reshaping,\n",
    "        # - scores is of shape (n_classes, n_folds, n_Cs . n_l1_ratios)\n",
    "        # - coefs_paths is of shape\n",
    "        #  (n_classes, n_folds, n_Cs . n_l1_ratios, n_features)\n",
    "        # - n_iter is of shape\n",
    "        #  (n_classes, n_folds, n_Cs . n_l1_ratios) or\n",
    "        #  (1, n_folds, n_Cs . n_l1_ratios)\n",
    "        coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)\n",
    "        self.Cs_ = Cs[0]\n",
    "        if multi_class == \"multinomial\":\n",
    "            coefs_paths = np.reshape(\n",
    "                coefs_paths,\n",
    "                (len(folds), len(l1_ratios_) * len(self.Cs_), n_classes, -1),\n",
    "            )\n",
    "            # equiv to coefs_paths = np.moveaxis(coefs_paths, (0, 1, 2, 3),\n",
    "            #                                                 (1, 2, 0, 3))\n",
    "            coefs_paths = np.swapaxes(coefs_paths, 0, 1)\n",
    "            coefs_paths = np.swapaxes(coefs_paths, 0, 2)\n",
    "            self.n_iter_ = np.reshape(\n",
    "                n_iter_, (1, len(folds), len(self.Cs_) * len(l1_ratios_))\n",
    "            )\n",
    "            # repeat same scores across all classes\n",
    "            scores = np.tile(scores, (n_classes, 1, 1))\n",
    "        else:\n",
    "            coefs_paths = np.reshape(\n",
    "                coefs_paths,\n",
    "                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_), -1),\n",
    "            )\n",
    "            self.n_iter_ = np.reshape(\n",
    "                n_iter_, (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_))\n",
    "            )\n",
    "        scores = np.reshape(scores, (n_classes, len(folds), -1))\n",
    "        self.scores_ = dict(zip(classes, scores))\n",
    "        self.coefs_paths_ = dict(zip(classes, coefs_paths))\n",
    "\n",
    "        self.C_ = list()\n",
    "        self.l1_ratio_ = list()\n",
    "        self.coef_ = np.empty((n_classes, X.shape[1]))\n",
    "        self.intercept_ = np.zeros(n_classes)\n",
    "        for index, (cls, encoded_label) in enumerate(\n",
    "            zip(iter_classes, iter_encoded_labels)\n",
    "        ):\n",
    "            if multi_class == \"ovr\":\n",
    "                scores = self.scores_[cls]\n",
    "                coefs_paths = self.coefs_paths_[cls]\n",
    "            else:\n",
    "                # For multinomial, all scores are the same across classes\n",
    "                scores = scores[0]\n",
    "                # coefs_paths will keep its original shape because\n",
    "                # logistic_regression_path expects it this way\n",
    "\n",
    "            if self.refit:\n",
    "                # best_index is between 0 and (n_Cs . n_l1_ratios - 1)\n",
    "                # for example, with n_cs=2 and n_l1_ratios=3\n",
    "                # the layout of scores is\n",
    "                # [c1, c2, c1, c2, c1, c2]\n",
    "                #   l1_1 ,  l1_2 ,  l1_3\n",
    "                best_index = scores.sum(axis=0).argmax()\n",
    "\n",
    "                best_index_C = best_index % len(self.Cs_)\n",
    "                C_ = self.Cs_[best_index_C]\n",
    "                self.C_.append(C_)\n",
    "\n",
    "                best_index_l1 = best_index // len(self.Cs_)\n",
    "                l1_ratio_ = l1_ratios_[best_index_l1]\n",
    "                self.l1_ratio_.append(l1_ratio_)\n",
    "\n",
    "                if multi_class == \"multinomial\":\n",
    "                    coef_init = np.mean(coefs_paths[:, :, best_index, :], axis=1)\n",
    "                else:\n",
    "                    coef_init = np.mean(coefs_paths[:, best_index, :], axis=0)\n",
    "\n",
    "                # Note that y is label encoded and hence pos_class must be\n",
    "                # the encoded label / None (for 'multinomial')\n",
    "                w, _, _ = _logistic_regression_path(\n",
    "                    X,\n",
    "                    y,\n",
    "                    pos_class=encoded_label,\n",
    "                    Cs=[C_],\n",
    "                    solver=solver,\n",
    "                    fit_intercept=self.fit_intercept,\n",
    "                    coef=coef_init,\n",
    "                    max_iter=self.max_iter,\n",
    "                    tol=self.tol,\n",
    "                    penalty=self.penalty,\n",
    "                    class_weight=class_weight,\n",
    "                    multi_class=multi_class,\n",
    "                    verbose=max(0, self.verbose - 1),\n",
    "                    random_state=self.random_state,\n",
    "                    check_input=False,\n",
    "                    max_squared_sum=max_squared_sum,\n",
    "                    sample_weight=sample_weight,\n",
    "                    l1_ratio=l1_ratio_,\n",
    "                )\n",
    "                w = w[0]\n",
    "\n",
    "            else:\n",
    "                # Take the best scores across every fold and the average of\n",
    "                # all coefficients corresponding to the best scores.\n",
    "                best_indices = np.argmax(scores, axis=1)\n",
    "                if multi_class == \"ovr\":\n",
    "                    w = np.mean(\n",
    "                        [coefs_paths[i, best_indices[i], :] for i in range(len(folds))],\n",
    "                        axis=0,\n",
    "                    )\n",
    "                else:\n",
    "                    w = np.mean(\n",
    "                        [\n",
    "                            coefs_paths[:, i, best_indices[i], :]\n",
    "                            for i in range(len(folds))\n",
    "                        ],\n",
    "                        axis=0,\n",
    "                    )\n",
    "\n",
    "                best_indices_C = best_indices % len(self.Cs_)\n",
    "                self.C_.append(np.mean(self.Cs_[best_indices_C]))\n",
    "\n",
    "                if self.penalty == \"elasticnet\":\n",
    "                    best_indices_l1 = best_indices // len(self.Cs_)\n",
    "                    self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))\n",
    "                else:\n",
    "                    self.l1_ratio_.append(None)\n",
    "\n",
    "            if multi_class == \"multinomial\":\n",
    "                self.C_ = np.tile(self.C_, n_classes)\n",
    "                self.l1_ratio_ = np.tile(self.l1_ratio_, n_classes)\n",
    "                self.coef_ = w[:, : X.shape[1]]\n",
    "                if self.fit_intercept:\n",
    "                    self.intercept_ = w[:, -1]\n",
    "            else:\n",
    "                self.coef_[index] = w[: X.shape[1]]\n",
    "                if self.fit_intercept:\n",
    "                    self.intercept_[index] = w[-1]\n",
    "\n",
    "        self.C_ = np.asarray(self.C_)\n",
    "        self.l1_ratio_ = np.asarray(self.l1_ratio_)\n",
    "        self.l1_ratios_ = np.asarray(l1_ratios_)\n",
    "        # if elasticnet was used, add the l1_ratios dimension to some\n",
    "        # attributes\n",
    "        if self.l1_ratios is not None:\n",
    "            # with n_cs=2 and n_l1_ratios=3\n",
    "            # the layout of scores is\n",
    "            # [c1, c2, c1, c2, c1, c2]\n",
    "            #   l1_1 ,  l1_2 ,  l1_3\n",
    "            # To get a 2d array with the following layout\n",
    "            #      l1_1, l1_2, l1_3\n",
    "            # c1 [[ .  ,  .  ,  .  ],\n",
    "            # c2  [ .  ,  .  ,  .  ]]\n",
    "            # We need to first reshape and then transpose.\n",
    "            # The same goes for the other arrays\n",
    "            for cls, coefs_path in self.coefs_paths_.items():\n",
    "                self.coefs_paths_[cls] = coefs_path.reshape(\n",
    "                    (len(folds), self.l1_ratios_.size, self.Cs_.size, -1)\n",
    "                )\n",
    "                self.coefs_paths_[cls] = np.transpose(\n",
    "                    self.coefs_paths_[cls], (0, 2, 1, 3)\n",
    "                )\n",
    "            for cls, score in self.scores_.items():\n",
    "                self.scores_[cls] = score.reshape(\n",
    "                    (len(folds), self.l1_ratios_.size, self.Cs_.size)\n",
    "                )\n",
    "                self.scores_[cls] = np.transpose(self.scores_[cls], (0, 2, 1))\n",
    "\n",
    "            self.n_iter_ = self.n_iter_.reshape(\n",
    "                (-1, len(folds), self.l1_ratios_.size, self.Cs_.size)\n",
    "            )\n",
    "            self.n_iter_ = np.transpose(self.n_iter_, (0, 1, 3, 2))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def score(self, X, y, sample_weight=None, **score_params):\n",
    "        \"\"\"Score using the `scoring` option on the given test data and labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Test samples.\n",
    "\n",
    "        y : array-like of shape (n_samples,)\n",
    "            True labels for X.\n",
    "\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights.\n",
    "\n",
    "        **score_params : dict\n",
    "            Parameters to pass to the `score` method of the underlying scorer.\n",
    "\n",
    "            .. versionadded:: 1.4\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            Score of self.predict(X) w.r.t. y.\n",
    "        \"\"\"\n",
    "        _raise_for_params(score_params, self, \"score\")\n",
    "\n",
    "        scoring = self._get_scorer()\n",
    "        if _routing_enabled():\n",
    "            routed_params = process_routing(\n",
    "                self,\n",
    "                \"score\",\n",
    "                sample_weight=sample_weight,\n",
    "                **score_params,\n",
    "            )\n",
    "        else:\n",
    "            routed_params = Bunch()\n",
    "            routed_params.scorer = Bunch(score={})\n",
    "            if sample_weight is not None:\n",
    "                routed_params.scorer.score[\"sample_weight\"] = sample_weight\n",
    "\n",
    "        return scoring(\n",
    "            self,\n",
    "            X,\n",
    "            y,\n",
    "            **routed_params.scorer.score,\n",
    "        )\n",
    "\n",
    "    def get_metadata_routing(self):\n",
    "        \"\"\"Get metadata routing of this object.\n",
    "\n",
    "        Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
    "        mechanism works.\n",
    "\n",
    "        .. versionadded:: 1.4\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        routing : MetadataRouter\n",
    "            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
    "            routing information.\n",
    "        \"\"\"\n",
    "\n",
    "        router = (\n",
    "            MetadataRouter(owner=self.__class__.__name__)\n",
    "            .add_self_request(self)\n",
    "            .add(\n",
    "                splitter=self.cv,\n",
    "                method_mapping=MethodMapping().add(caller=\"fit\", callee=\"split\"),\n",
    "            )\n",
    "            .add(\n",
    "                scorer=self._get_scorer(),\n",
    "                method_mapping=MethodMapping()\n",
    "                .add(caller=\"score\", callee=\"score\")\n",
    "                .add(caller=\"fit\", callee=\"score\"),\n",
    "            )\n",
    "        )\n",
    "        return router\n",
    "\n",
    "    def _get_scorer(self):\n",
    "        \"\"\"Get the scorer based on the scoring method specified.\n",
    "        The default scoring method is `accuracy`.\n",
    "        \"\"\"\n",
    "        scoring = self.scoring or \"accuracy\"\n",
    "        return get_scorer(scoring)\n",
    "\n",
    "    def __sklearn_tags__(self):\n",
    "        tags = super().__sklearn_tags__()\n",
    "        tags.input_tags.sparse = True\n",
    "        return tags\n",
    "\n",
    "```"
   ],
   "id": "e752783dbba6444e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ae8af4c6fecfa259"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "下面我**从零重新讲解**你给的 `LogisticRegressionCV` 源码，按“初学者能理解”的方式，把每一段代码在做什么、为什么这么做、最终影响什么属性，讲清楚。为了不把信息一次性塞爆，我把讲解组织成 **A. 类与参数系统 → B. fit 主流程 → C. refit/选参细节 → D. score 与 metadata routing → E. 训练后你能用到哪些属性**。\n",
    "\n",
    "---\n",
    "\n",
    "# A. 这个类到底是什么？\n",
    "\n",
    "```python\n",
    "class LogisticRegressionCV(LogisticRegression, LinearClassifierMixin, BaseEstimator):\n",
    "```\n",
    "\n",
    "它是一个“**带交叉验证自动调参**”的逻辑回归模型。\n",
    "\n",
    "* 继承 `LogisticRegression`：复用逻辑回归的多数能力（例如一些通用行为、标签系统等）。\n",
    "* `LinearClassifierMixin`：提供线性分类器通用接口（例如决策函数相关行为）。\n",
    "* `BaseEstimator`：让它符合 sklearn 生态规范（能 `get_params/set_params`、能进 Pipeline、能被 clone）。\n",
    "\n",
    "你可以把它当成：\n",
    "\n",
    "> **LogisticRegression + 内置 CV 找最佳 C / l1_ratio +（可选）用最佳参数在全量数据上再训练一次（refit）**\n",
    "\n",
    "---\n",
    "\n",
    "# B. 参数校验系统 `_parameter_constraints`：为什么要先删掉一些再加回来？\n",
    "\n",
    "```python\n",
    "_parameter_constraints: dict = {**LogisticRegression._parameter_constraints}\n",
    "\n",
    "for param in [\"C\", \"warm_start\", \"l1_ratio\"]:\n",
    "    _parameter_constraints.pop(param)\n",
    "\n",
    "_parameter_constraints.update({...})\n",
    "```\n",
    "\n",
    "## 1）先复制父类的“参数合法性规则”\n",
    "\n",
    "`LogisticRegression` 本来有一套参数约束，比如 `C` 必须是正数等。\n",
    "\n",
    "## 2）为什么删掉 `C / warm_start / l1_ratio`？\n",
    "\n",
    "因为在 **CV 版本**里：\n",
    "\n",
    "* 不用单个 `C`，而是用 **`Cs`（一组候选 C）**\n",
    "* 不用单个 `l1_ratio`，而是用 **`l1_ratios`（一组候选 l1_ratio）**\n",
    "* `warm_start` 在 CV/并行路径里处理方式不同，因此不作为“外部参数”直接约束\n",
    "\n",
    "## 3）新增本类约束\n",
    "\n",
    "它为 `Cs/cv/scoring/l1_ratios/refit/penalty` 增加合法性说明。\n",
    "初学者记住用途就行：\n",
    "\n",
    "> **这段代码是为了让 sklearn 在你传错参数时更早、更清晰地报错。**\n",
    "\n",
    "---\n",
    "\n",
    "# C. `__init__`：只存参数，不训练\n",
    "\n",
    "```python\n",
    "def __init__(..., Cs=10, fit_intercept=True, cv=None, ...):\n",
    "    self.Cs = Cs\n",
    "    self.fit_intercept = fit_intercept\n",
    "    ...\n",
    "```\n",
    "\n",
    "`__init__` 只做一件事：**把你传入的配置保存到 self 上**。\n",
    "真正训练在 `.fit()`。\n",
    "\n",
    "---\n",
    "\n",
    "# D. `fit()`：核心主流程（逐段拆解）\n",
    "\n",
    "```python\n",
    "@_fit_context(prefer_skip_nested_validation=True)\n",
    "def fit(self, X, y, sample_weight=None, **params):\n",
    "```\n",
    "\n",
    "`fit()` 是全类最重要的函数。你可以把它理解为：\n",
    "\n",
    "> **检查参数 → 规范化数据 → 准备 CV 划分 → 并行训练/评分（遍历所有超参组合）→ 汇总 → 选最佳 →（可选）全量重训 → 填好模型属性**\n",
    "\n",
    "下面按源码顺序讲。\n",
    "\n",
    "---\n",
    "\n",
    "## D1. 额外参数检查\n",
    "\n",
    "```python\n",
    "_raise_for_params(params, self, \"fit\")\n",
    "```\n",
    "\n",
    "`fit()` 从 sklearn 1.4 开始支持把一些参数“路由”给 splitter/scorer。\n",
    "这里先检查 `**params` 里有没有不允许的内容，避免“悄悄忽略”。\n",
    "\n",
    "---\n",
    "\n",
    "## D2. 检查 solver 与 penalty/dual 是否兼容\n",
    "\n",
    "```python\n",
    "solver = _check_solver(self.solver, self.penalty, self.dual)\n",
    "```\n",
    "\n",
    "逻辑回归不同 solver 支持的 penalty 不同，例如：\n",
    "\n",
    "* `lbfgs/newton-cg/sag`：一般只支持 L2\n",
    "* `saga`：支持 l1/l2/elasticnet\n",
    "* `liblinear`：支持 l1/l2，但 multinomial 有限制\n",
    "\n",
    "这一步是“**预防性报错**”：不合法组合直接停。\n",
    "\n",
    "---\n",
    "\n",
    "## D3. ElasticNet 情况必须提供 `l1_ratios`\n",
    "\n",
    "```python\n",
    "if self.penalty == \"elasticnet\":\n",
    "    if self.l1_ratios is None or len(self.l1_ratios)==0 or any(不在[0,1]):\n",
    "        raise ValueError(...)\n",
    "    l1_ratios_ = self.l1_ratios\n",
    "else:\n",
    "    if self.l1_ratios is not None:\n",
    "        warnings.warn(...)\n",
    "    l1_ratios_ = [None]\n",
    "```\n",
    "\n",
    "关键点：\n",
    "\n",
    "* elasticnet 需要 `l1_ratio`（L1 与 L2 的混合比例）\n",
    "* 非 elasticnet 时，`l1_ratios` 不起作用 → 给 warning\n",
    "* 把 `l1_ratios_` 统一成“列表形式”（即使不是 elasticnet 也设为 `[None]`），这样后面写循环不用分支太多\n",
    "\n",
    "---\n",
    "\n",
    "## D4. 输入数据标准化（非常重要）\n",
    "\n",
    "```python\n",
    "X, y = validate_data(\n",
    "    self, X, y,\n",
    "    accept_sparse=\"csr\",\n",
    "    dtype=np.float64,\n",
    "    order=\"C\",\n",
    "    accept_large_sparse=solver not in [\"liblinear\",\"sag\",\"saga\"],\n",
    ")\n",
    "check_classification_targets(y)\n",
    "```\n",
    "\n",
    "这一步做的是：\n",
    "\n",
    "* 把 `X` 转成 solver 更喜欢的数据格式：\n",
    "\n",
    "  * 稀疏矩阵允许，但最好 CSR\n",
    "  * 统一用 float64（优化算法更稳）\n",
    "  * 内存布局尽量连续（更快）\n",
    "* `check_classification_targets(y)`：确保你的 `y` 是分类标签而不是回归值\n",
    "\n",
    "---\n",
    "\n",
    "## D5. 标签编码（字符串标签 → 整数标签）\n",
    "\n",
    "```python\n",
    "label_encoder = LabelEncoder().fit(y)\n",
    "y = label_encoder.transform(y)\n",
    "```\n",
    "\n",
    "比如：`[\"cat\",\"dog\",\"cat\"] → [0,1,0]`\n",
    "这样内部计算更方便。\n",
    "\n",
    "如果你传了 `class_weight` 的字典（用原始标签当 key），它也要同步映射：\n",
    "\n",
    "```python\n",
    "if isinstance(class_weight, dict):\n",
    "    class_weight = { encoded(cls): v for cls,v in class_weight.items() }\n",
    "```\n",
    "\n",
    "同时保存原始类别：\n",
    "\n",
    "```python\n",
    "self.classes_ = label_encoder.classes_\n",
    "```\n",
    "\n",
    "这很关键：预测时还能把数字类别映射回原标签。\n",
    "\n",
    "---\n",
    "\n",
    "## D6. multi_class 处理（带弃用 warning）\n",
    "\n",
    "源码里那一大段 warning 的本质是：\n",
    "\n",
    "* `multi_class` 正在被 sklearn 弃用（1.5 起），未来统一 multinomial\n",
    "* 这里为了兼容旧行为，最后会把它变成实际使用的 `multi_class`：\n",
    "\n",
    "```python\n",
    "multi_class = _check_multi_class(multi_class, solver, len(classes))\n",
    "```\n",
    "\n",
    "初学者记住结论：\n",
    "\n",
    "> 这一步决定使用 **OvR** 还是 **multinomial** 多分类训练方式。\n",
    "\n",
    "---\n",
    "\n",
    "## D7. sag/saga 的加速项 `max_squared_sum`\n",
    "\n",
    "```python\n",
    "if solver in [\"sag\",\"saga\"]:\n",
    "    max_squared_sum = row_norms(X, squared=True).max()\n",
    "else:\n",
    "    max_squared_sum = None\n",
    "```\n",
    "\n",
    "仅用于 sag/saga 的收敛/步长计算，属于性能细节。\n",
    "\n",
    "---\n",
    "\n",
    "## D8. metadata routing：把参数分给 splitter/scorer\n",
    "\n",
    "```python\n",
    "if _routing_enabled():\n",
    "    routed_params = process_routing(...)\n",
    "else:\n",
    "    routed_params = Bunch()\n",
    "    routed_params.splitter = Bunch(split={})\n",
    "    routed_params.scorer = Bunch(score=params)\n",
    "    if sample_weight is not None:\n",
    "        routed_params.scorer.score[\"sample_weight\"] = sample_weight\n",
    "```\n",
    "\n",
    "如果你暂时没用“高级路由”，可以这样理解：\n",
    "\n",
    "* 这一步把 `sample_weight` 等参数整理好\n",
    "* 之后 CV 的 `split`、评分的 `score` 都会用到它们\n",
    "\n",
    "---\n",
    "\n",
    "## D9. 生成交叉验证 folds\n",
    "\n",
    "```python\n",
    "cv = check_cv(self.cv, y, classifier=True)\n",
    "folds = list(cv.split(X, y, **routed_params.splitter.split))\n",
    "```\n",
    "\n",
    "`folds` 是一个列表，每个元素是 `(train_index, test_index)`。\n",
    "\n",
    "---\n",
    "\n",
    "## D10. 类别数处理（尤其是二分类）\n",
    "\n",
    "```python\n",
    "n_classes = len(encoded_labels)\n",
    "if n_classes < 2: raise ValueError(...)\n",
    "if n_classes == 2:\n",
    "    n_classes = 1\n",
    "    encoded_labels = encoded_labels[1:]\n",
    "    classes = classes[1:]\n",
    "```\n",
    "\n",
    "为什么二分类要变成 `n_classes=1`？\n",
    "\n",
    "* 二分类在 OvR 视角下，只需要“拟合一个正类 vs 负类”的模型就够了\n",
    "* 这样后面循环只跑一次，省一半计算\n",
    "\n",
    "---\n",
    "\n",
    "## D11. multinomial 的 “只迭代一次” hack\n",
    "\n",
    "```python\n",
    "if multi_class == \"multinomial\":\n",
    "    iter_encoded_labels = iter_classes = [None]\n",
    "else:\n",
    "    iter_encoded_labels = encoded_labels\n",
    "    iter_classes = classes\n",
    "```\n",
    "\n",
    "* OvR：按类循环（每类一个二分类器）\n",
    "* multinomial：一次拟合所有类 → 不需要按类循环\n",
    "  所以给 `[None]`，保持循环结构统一。\n",
    "\n",
    "---\n",
    "\n",
    "## D12. class_weight='balanced' 的计算\n",
    "\n",
    "```python\n",
    "if class_weight == \"balanced\":\n",
    "    class_weight = compute_class_weight(...)\n",
    "    class_weight = dict(enumerate(class_weight))\n",
    "```\n",
    "\n",
    "这是 sklearn 的标准“按类频率反比”权重计算。\n",
    "\n",
    "---\n",
    "\n",
    "## D13. 并行跑 `_log_reg_scoring_path`（最核心）\n",
    "\n",
    "```python\n",
    "path_func = delayed(_log_reg_scoring_path)\n",
    "\n",
    "prefer = \"threads\" if self.solver in [\"sag\",\"saga\"] else \"processes\"\n",
    "\n",
    "fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
    "    path_func(\n",
    "        X, y,\n",
    "        train, test,\n",
    "        pos_class=label,\n",
    "        Cs=self.Cs,\n",
    "        ...\n",
    "        l1_ratio=l1_ratio,\n",
    "        score_params=routed_params.scorer.score,\n",
    "    )\n",
    "    for label in iter_encoded_labels\n",
    "    for train, test in folds\n",
    "    for l1_ratio in l1_ratios_\n",
    ")\n",
    "```\n",
    "\n",
    "这一段你可以用一句话理解：\n",
    "\n",
    "> 对每个 **(label 或 multinomial 的 None)** × 每个 **fold** × 每个 **l1_ratio**，\n",
    "> 计算一条“沿着多个 `C` 值的训练与评分路径”，并把结果收集起来。\n",
    "\n",
    "### 为什么叫 path？\n",
    "\n",
    "因为 `_log_reg_scoring_path` 会在一组 `Cs` 上连续训练/评估（通常可以 warm-start），得到：\n",
    "\n",
    "* 每个 C 对应的系数（coefs）\n",
    "* 每个 C 对应的评分（scores）\n",
    "* 迭代次数等\n",
    "\n",
    "### 为什么 sag/saga 用 threads？\n",
    "\n",
    "因为 sag/saga 释放 GIL，多线程更划算；其他 solver 用进程并行更常见。\n",
    "\n",
    "---\n",
    "\n",
    "# E. 汇总结果：reshape 成你能读懂的结构\n",
    "\n",
    "并行返回的 `fold_coefs_` 是一个“扁平列表”。\n",
    "源码接下来做的是把它**恢复成有意义的多维数组**：\n",
    "\n",
    "```python\n",
    "coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)\n",
    "self.Cs_ = Cs[0]\n",
    "...\n",
    "self.scores_ = dict(zip(classes, scores))\n",
    "self.coefs_paths_ = dict(zip(classes, coefs_paths))\n",
    "```\n",
    "\n",
    "* `scores_`：每个类对应一个数组，里面是 “每折 × 每个超参组合”的分数\n",
    "* `coefs_paths_`：每个类对应一个数组，里面是 “每折 × 每个超参组合”的系数路径\n",
    "* `n_iter_`：记录迭代次数\n",
    "\n",
    "multinomial 与 ovr 的输出形状不一致，所以你看到很多 `reshape` / `swapaxes`：本质是**把维度对齐**。\n",
    "\n",
    "---\n",
    "\n",
    "# F. 选最优超参 + 生成最终 `coef_ / intercept_`\n",
    "\n",
    "初始化最终输出：\n",
    "\n",
    "```python\n",
    "self.C_ = list()\n",
    "self.l1_ratio_ = list()\n",
    "self.coef_ = np.empty((n_classes, X.shape[1]))\n",
    "self.intercept_ = np.zeros(n_classes)\n",
    "```\n",
    "\n",
    "然后对每个“要拟合的类”（OvR）或“一次 multinomial”循环：\n",
    "\n",
    "## F1. `refit=True`：先选最优，再全量重训（最常用）\n",
    "\n",
    "```python\n",
    "best_index = scores.sum(axis=0).argmax()\n",
    "best_index_C = best_index % len(self.Cs_)\n",
    "C_ = self.Cs_[best_index_C]\n",
    "best_index_l1 = best_index // len(self.Cs_)\n",
    "l1_ratio_ = l1_ratios_[best_index_l1]\n",
    "```\n",
    "\n",
    "* `scores` 形状：`(n_folds, n_candidates)`\n",
    "* `scores.sum(axis=0)`：把每个候选参数组合在所有折上汇总（相当于平均表现）\n",
    "* `argmax()`：取总体最好的那组参数\n",
    "\n",
    "接着用 CV 得到的系数路径做一个“合理的初始值”（warm start 的思路）：\n",
    "\n",
    "```python\n",
    "coef_init = np.mean(...coefs_paths..., axis=...)\n",
    "```\n",
    "\n",
    "最后用全量数据重新训练一次，只训练最佳参数那一个点：\n",
    "\n",
    "```python\n",
    "w, _, _ = _logistic_regression_path(\n",
    "    X, y, Cs=[C_], coef=coef_init, l1_ratio=l1_ratio_, ...\n",
    ")\n",
    "w = w[0]\n",
    "```\n",
    "\n",
    "并写入最终参数：\n",
    "\n",
    "* OvR：`self.coef_[index] = w[:n_features]`，截距 `w[-1]`\n",
    "* multinomial：`w` 可能是 `(n_classes, n_features(+1))` 的结构\n",
    "\n",
    "## F2. `refit=False`：不重训，直接平均各折最优参数\n",
    "\n",
    "它会对每一折先找最优超参索引，然后把对应系数平均起来作为最终 `w`。\n",
    "这个模式更像“折内最优的集合平均”，但一般不如 refit 直观。\n",
    "\n",
    "---\n",
    "\n",
    "# G. ElasticNet 情况：把扁平候选组合还原成网格\n",
    "\n",
    "如果使用了 `l1_ratios`，为了让结果更好理解，它把原本扁平排列的：\n",
    "\n",
    "`[c1,c2,c1,c2,c1,c2] (对应 l1_1,l1_2,l1_3)`\n",
    "整理成二维网格形式（行 C，列 l1_ratio），因此又做了一轮 reshape + transpose。\n",
    "\n",
    "---\n",
    "\n",
    "# H. `score()`：按 scoring 规则打分\n",
    "\n",
    "```python\n",
    "scoring = self._get_scorer()\n",
    "return scoring(self, X, y, **routed_params.scorer.score)\n",
    "```\n",
    "\n",
    "* 默认 scoring 是 accuracy\n",
    "* 你也可以设 `scoring=\"f1\" / \"roc_auc\" / 自定义 callable`\n",
    "\n",
    "---\n",
    "\n",
    "# I. `get_metadata_routing()`：告诉 sklearn 参数如何路由\n",
    "\n",
    "这段是 sklearn 1.4+ 的机制：\n",
    "\n",
    "* fit 里某些参数给 splitter.split\n",
    "* score/fit 里某些参数给 scorer.score\n",
    "\n",
    "初学者阶段知道“它是工程机制，保证和 Pipeline/元估计器兼容”即可。\n",
    "\n",
    "---\n",
    "\n",
    "# J. `__sklearn_tags__()`：声明支持稀疏输入\n",
    "\n",
    "```python\n",
    "tags.input_tags.sparse = True\n",
    "```\n",
    "\n",
    "告诉 sklearn：这个 estimator 可以吃稀疏矩阵（CSR）。\n",
    "\n",
    "---\n",
    "\n",
    "# K. 你训练完最该看哪些属性？\n",
    "\n",
    "* `classes_`：原始类标签顺序\n",
    "* `Cs_`：实际使用的 C 网格\n",
    "* `scores_`：每类在各折、各超参下的分数（调参可视化就看它）\n",
    "* `C_`：最终选出来的最佳 C（refit=True 时）\n",
    "* `l1_ratio_`：最终选出来的最佳 l1_ratio（elasticnet 时）\n",
    "* `coef_ / intercept_`：最终模型参数\n",
    "* `coefs_paths_`：系数随超参变化的路径（分析正则化影响很有用）\n",
    "* `n_iter_`：迭代次数\n",
    "\n",
    "---\n",
    "\n",
    "## 你接下来最容易卡住的 2 个点（我可以继续深入讲）\n",
    "\n",
    "1. `_log_reg_scoring_path` 与 `_logistic_regression_path` 的返回值结构到底是什么？为什么能一次算一串 Cs？\n",
    "2. multinomial vs ovr 的 `reshape/swapaxes` 到底在对齐哪些维度？（我可以用一个“2 折 × 3 个 C × 2 类”的小例子把形状一步步写出来）\n",
    "\n",
    "你如果希望我继续，我建议下一步从 **`fold_coefs_` 的内容长什么样**开始，把“并行返回结果→reshape→最终属性”这一段彻底讲透。\n"
   ],
   "id": "8f82c369fac469f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
